import re


def tokenize(text: str) -> list[str]:
    pattern = r"[\w]+(?:-[\w]+)*"
    tokens = []
    for match in re.finditer(pattern, text):
        tokens.append(match.group())

    return tokens


test_cases = [
    "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä-=",
    "hello,world!!!",
    "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ",
    "2025 –≥–æ–¥",
    "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ",
]
for test in test_cases:
    result = tokenize(test)
    print(result)
