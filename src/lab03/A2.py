import re
def tokenize(text: str) -> list[str]:
    pattern = (r'[\w-]+')
    tokens = []
    for match in re.finditer(pattern , text):
        tokens.append(match.group())

    return tokens
test_cases = ["–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", "hello,world!!!", "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", "2025 –≥–æ–¥", "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"]
for test in test_cases:
    result = tokenize(test)
    print(result)